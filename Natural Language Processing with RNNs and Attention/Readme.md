# Natural Language Processing with RNNs

The goal of NLP research is to build machines that master language, though it typically focuses on specific tasks like text classification, translation, summarization, and sentiment analysis.

The passage outlines an exploration of recurrent neural networks (RNNs) in NLP, starting with:

Char-RNNs: Predicting the next character to generate text, using stateless and stateful RNNs.
Word-level RNNs: Performing tasks like sentiment analysis.
Encoder-decoder architectures: For neural machine translation (e.g., English to Spanish).
It then introduces attention mechanisms, which help models focus on relevant input parts, enhancing RNN-based architectures. Finally, it transitions to transformers, attention-only models like GPT and BERT, which have revolutionized NLP. The chapter concludes by exploring Hugging Face's Transformers library and demonstrating a Shakespeare-style text generation model.

## Generating Shakespearean Text with a Character RNN
Andrej Karpathy demonstrated that a character RNN (char-RNN) could generate text by predicting the next character in a sequence. This approach is applied to Shakespeare's works, training an RNN to learn patterns in text. The process involves creating a dataset of text sequences, building a model with an embedding layer and GRU, training it to predict the next character, and generating text that mimics Shakespeare's style.

Here‚Äôs the corrected and well-organized code for creating, training, and using a character RNN to generate Shakespearean text.

# Reusing Pretrained Embeddings and Language Models in NLP
## Pretrained Word Embeddings:

Word embeddings like Google‚Äôs Word2vec, Stanford‚Äôs GloVe, and Facebook‚Äôs FastText are trained on large corpora and capture general meanings of words.
## Limitation: 
Words have a single representation, regardless of context (e.g., "right" in "left and right" vs. "right and wrong").
Contextualized Word Embeddings:

Introduced by ELMo in 2018, these embeddings consider the context in which a word is used.
ELMo uses internal states of a deep bidirectional language model for richer representations.

## Language Model Pretraining:

ULMFiT (2018): Demonstrated the effectiveness of unsupervised pretraining and fine-tuning for NLP tasks.

## Key achievements:
Significant improvements in text classification tasks (18‚Äì24% error reduction).
Fine-tuned models with minimal labeled data can outperform models trained from scratch with large datasets.
This marked a shift in NLP, making pretrained models the norm.


## Transformer-Based Pretrained Models:

Universal Sentence Encoder (2018): A Google transformer-based architecture available on TensorFlow Hub.

## Example implementation:
The Universal Sentence Encoder is fine-tuned alongside additional dense layers for sentiment analysis.
The model achieves over 90% validation accuracy, comparable to human performance on ambiguous reviews.

## Practical Usage:

Pretrained embeddings and language models like the Universal Sentence Encoder can significantly improve model performance with minimal labeled data.
Models are often fine-tuned during training for task-specific improvements, leveraging the general language understanding learned from massive corpora.


# The Encoder-Decoder Network for Neural Machine Translation (NMT) model:

## Overview
This simple NMT model translates English sentences to Spanish using an Encoder-Decoder architecture with LSTM layers.

## Key Concepts
Encoder: Processes the English sentence into a hidden state.
Decoder: Generates the Spanish sentence based on the encoder's hidden state.
Teacher Forcing: During training, the decoder is fed the correct previous word instead of its own output, speeding up training.
Start-of-Sequence (SOS) & End-of-Sequence (EOS) Tokens:
SOS token is the initial input for the decoder.
EOS token marks the sentence's end.
Process

## Data Preparation:

Download an English-Spanish sentence pair dataset.
Preprocess: Remove unnecessary characters, shuffle, and split data into training and validation sets.
Tokenize sentences using two TextVectorization layers:
One for English sentences.
One for Spanish sentences (with SOS and EOS tokens).
Model Architecture:

## Encoder:
Text is tokenized and embedded using an Embedding layer.
A single LSTM layer generates the encoder's hidden states.

## Decoder:

Takes Spanish inputs (with SOS token) and generates embeddings.
LSTM layer processes embeddings, initialized with the encoder's hidden states.
Dense layer with softmax activation predicts probabilities for each word in the Spanish vocabulary.


## Training:

Loss: sparse_categorical_crossentropy
Optimizer: Nadam
Metrics: Accuracy
Training: Use teacher forcing by shifting target Spanish sentences during training.

## Inference:

During prediction, the decoder uses its previous output (not the ground truth) as input.
A utility function iteratively predicts one word at a time until reaching the EOS token.
Example Translation:
Input: "I like soccer"
Output: "me gusta el f√∫tbol"

## Improvements:
Increase dataset size.
Use deeper architectures with more LSTM layers.
Explore advanced techniques like bidirectional layers for better performance.
This basic model is functional but struggles with long or complex sentences, highlighting the need for further refinements.

This detailed explanation of sentiment analysis provides an excellent overview of applying natural language processing techniques for classifying IMDb reviews. Here are key points and enhancements to focus on:

## Understanding the Dataset
Dataset Description: IMDb reviews dataset includes 50,000 labeled movie reviews (positive or negative), split equally into training and testing sets.
Why Popular: Simple yet challenging enough for meaningful learning on NLP techniques.
## Data Loading and Preprocessing
TensorFlow Datasets (tfds) simplifies loading the IMDb dataset. The dataset is split into training (90%), validation (10%), and test sets.
Reviews are tokenized using the TextVectorization layer. While English reviews work well with space-based tokenization, subword techniques like BPE or WordPiece are better for languages with complex tokenization.
## Model Construction
Layers:
TextVectorization: Converts text to numerical tokens.
Embedding: Maps tokens to dense vector representations.
GRU: Captures temporal dependencies in text.
Dense: Outputs a binary classification (positive/negative sentiment).
Compilation:
Loss: binary_crossentropy.
Optimizer: nadam.
Metrics: Accuracy.
## Addressing Padding Issues
Problem: Long sequences padded with zeros can overwhelm the GRU's short-term memory.
Solution:
Use mask_zero=True in the Embedding layer to ignore padding tokens during training.
Alternatively, handle sequences using ragged tensors to represent data without padding.
## Masking for Advanced Models
Layers like RNN, GRU, LSTM, etc., support masking directly.
For custom layers, you can implement call() with a mask argument and optionally propagate the mask using supports_masking=True.
## Functional API and Dropout
More flexible model definition using the functional API.
Add dropout layers to prevent overfitting.
## Ragged Tensors for Variable-Length Sequences
Ragged tensors avoid padding by representing sequences of different lengths efficiently.
Supported directly by Keras's recurrent layers, simplifying implementation.
## Experimentation Tips
Hyperparameter Tuning:
Vocabulary size, embedding dimensions, and GRU units can significantly impact performance.
## Regularization
Techniques like dropout and early stopping can prevent overfitting.
Extended Tokenization:
Subword techniques (e.g., BPE) can enhance generalization for rare words.
##  Potential Improvements
Use pre-trained embeddings (e.g., GloVe or FastText) for better initial representations.
Explore advanced architectures like bidirectional GRUs/LSTMs or Transformer-based models for further accuracy improvements.

# Reusing Pretrained Embeddings and Language Models

Pretrained Word Embeddings:

Previously, word embeddings like Word2Vec, GloVe, and FastText, trained on large text corpora, were widely used for tasks like sentiment analysis. These embeddings cluster similar words (e.g., "awesome" and "amazing") in the embedding space.
Limitation: They provide a single representation per word, ignoring contextual differences (e.g., "right" in "left and right" vs. "right and wrong").
## Contextualized Embeddings:

ELMo (2018): Introduced contextualized embeddings by leveraging deep bidirectional language models to account for context-specific word meanings.
ULMFiT (2018): Demonstrated the power of unsupervised pretraining using LSTM-based language models. Fine-tuning pretrained models reduced error rates on text classification tasks significantly, marking the start of pretrained models dominating NLP.

## Pretrained Language Models:

Today, reusing pretrained models (e.g., transformers) is standard in NLP. These models excel in generalizing across tasks and domains.
Universal Sentence Encoder (USE):

A transformer-based model by Google, available via TensorFlow Hub, can be fine-tuned for specific tasks like sentiment analysis.


Models like USE can achieve over 90% validation accuracy on sentiment analysis tasks, rivaling human performance, particularly on ambiguous reviews. 

Encoder-Decoder Network for Neural Machine Translation (NMT)
Overview:
An Encoder-Decoder network is designed to translate English sentences to Spanish. The model uses teacher forcing during training, where the decoder is fed the target word from the previous step, enabling faster and more effective training. At inference time, the model predicts translations word by word using its prior outputs.

# Steps to Build the Model:

## Dataset Preparation:

Download the English-Spanish sentence pairs dataset.
Preprocess data by removing special Spanish characters (¬°, ¬ø) and splitting sentences into English and Spanish lists.
Shuffle and split data into training and validation sets.
Text Vectorization:

Create two TextVectorization layers, one for each language.
Limit vocabulary size to 1,000 for simplicity and efficiency.
Set sequence length to 50 tokens for uniformity.
Add "startofseq" (SOS) and "endofseq" (EOS) tokens for Spanish sentences.
Model Architecture:

Encoder: An embedding layer followed by a single LSTM layer. It encodes the English sentence and outputs the final LSTM states.
Decoder: Another embedding layer followed by an LSTM. The decoder uses the encoder's final state as its initial state.
Output Layer: A dense layer with softmax activation outputs probabilities for each word in the Spanish vocabulary.
Training:

Use sparse_categorical_crossentropy as the loss function.
Train the model using paired inputs: English sentences for the encoder and Spanish sentences prefixed with "startofseq" for the decoder.
Target sentences are shifted to include the "endofseq" token.
Translation at Inference Time:

The decoder sequentially generates one word at a time, using the previous output as input until the "endofseq" token is reached.
A utility function predicts translations iteratively for each word.
Limitations:

The model struggles with longer or more complex sentences due to limited data, vocabulary size, and model depth.


# Improvements:

Increase training data size.
Use more sophisticated architectures, like bidirectional LSTMs or attention mechanisms.
Expand the vocabulary size and incorporate pre-trained embeddings.

### Example:

Input: "I like soccer"
Output: "me gusta el f√∫tbol"
For longer sentences like "I like soccer and also going to the beach", the model may misinterpret due to its simplicity, highlighting the need for further enhancements.

Bidirectional RNNs and Beam Search

### Bidirectional RNNs:

Causal limitation of regular RNNs: They process inputs sequentially, only considering past and present information, which is suitable for tasks like forecasting or sequence decoding.

### Bidirectional RNNs: 
For tasks like text classification or sequence encoding, they use two RNN layers:
One processes inputs left-to-right.
The other processes inputs right-to-left.
Outputs from both layers are combined (e.g., concatenated) at each time step, allowing the model to consider both past and future context.

### Challenge in sequence-to-sequence models:

Bidirectional layers produce four states (forward and backward short-term and long-term states), while the decoder expects two (short-term and long-term states).

Solution: Concatenate the forward and backward states to create two states compatible with the decoder.

### Beam Search:

Problem: Standard encoder-decoder models may make irreversible mistakes while generating sequences.

### Solution - Beam Search:

Keeps track of the top k most likely sequences at each decoding step (beam width k).
Extends each sequence by one word, evaluates probabilities for all extensions, and retains only the k most probable sequences.

Ensures better translation accuracy by reconsidering earlier choices and retaining promising candidates.

### Example of Beam Search:

Start decoding with probabilities for the first word.
E.g., top candidates: "me" (75%), "a" (3%), "como" (1%).
Extend each candidate with the next word based on conditional probabilities.
For "me": "gustan" (36%), "gusta" (32%), "encanta" (16%).
Compute probabilities for all resulting two-word sentences (e.g., "me gustan" = 75% √ó 36% = 27%).
Keep the top k sentences.
Repeat the process for subsequent words until the sequence is complete.
E.g., "me gusta el f√∫tbol" emerges as the top translation over time.

### Limitations:
Beam Search improves performance but does not solve memory issues for long sequences in RNNs.
Attention mechanisms are needed to handle long-term dependencies effectively. 

# Attention Mechanisms and Transformers
## Attention Mechanisms
### Problem Addressed:

Traditional RNN-based encoder-decoder models need to carry context through many time steps, leading to information loss in long sentences.

## Introduction of Attention:

Allows the decoder to focus on relevant parts of the input sentence at each decoding step.
Shortens the effective path from input to output, reducing the impact of RNN memory limitations.

## How it Works:

The decoder uses a weighted sum of all encoder outputs at each time step.
Weights (
ùõº
Œ±) are computed by an alignment model (or attention layer) based on the decoder‚Äôs state and encoder outputs.
This mechanism dynamically determines the importance of each encoder output.

## Types of Attention:

## Bahdanau Attention (Additive):

Combines encoder outputs and decoder‚Äôs previous hidden state.
Scores are computed using a trainable function and passed through a softmax layer.

## Luong Attention (Multiplicative):
Uses the dot product for similarity computation, offering efficiency and simplicity.
Variants include "general" (transformed encoder outputs) and standard dot product mechanisms.

## Implementation in Keras:

tf.keras.layers.Attention: Implements Luong attention.
tf.keras.layers.AdditiveAttention: Implements Bahdanau attention.
Transformers: Attention Is All You Need

## Key Innovation:

A model architecture that uses only attention mechanisms (no RNNs or CNNs).
Revolutionized tasks like Neural Machine Translation (NMT).

## Advantages:

Avoids vanishing/exploding gradients.
Highly parallelizable, enabling efficient training.
Handles long-range dependencies better than RNNs.

## Architecture:

### Encoder:
Gradually transforms input embeddings into context-rich representations.
Captures word meanings in sentence context.

### Decoder:
Transforms each translated word into the representation of the next word.
Predicts output words using a Dense layer with softmax activation.

The explanation you've provided covers several key concepts in the development and evolution of transformer models, particularly focusing on the BERT model and its pretraining tasks (Masked Language Modeling and Next Sentence Prediction). It also touches on the subsequent innovations and applications of transformers in the field of Natural Language Processing (NLP) and computer vision.

Here's a summary of the key points discussed:

## BERT (Bidirectional Encoder Representations from Transformers):

BERT uses a bidirectional approach, allowing it to process both the left and right context of a word simultaneously, as opposed to the directional processing used by models like GPT.
Masked Language Model (MLM): Words in a sentence are randomly masked, and the model is trained to predict the masked words, which helps it learn bidirectional context.
Next Sentence Prediction (NSP): The model learns to predict whether two given sentences are consecutive. However, later research indicated that this task was less crucial than initially thought and has been omitted in many subsequent architectures.

## Training Approach:

BERT undergoes pretraining using these two tasks and is then fine-tuned for specific NLP tasks, such as text classification, where only the output token corresponding to the classification task is considered.
Fine-tuning is minimal, and the model adapts quickly to various tasks without needing substantial changes.
GPT-2 and the Rise of Larger Models:

GPT-2, with over 1.5 billion parameters, demonstrated the power of zero-shot learning (ZSL), achieving good performance on tasks without requiring fine-tuning.
This led to a trend of increasingly larger models, including Google‚Äôs Switch Transformers and the Wu Dao 2.0 model, pushing the boundaries of model size but also raising concerns about accessibility and environmental impact due to the high computational cost.

## Distillation and Efficient Transformers:

DistilBERT is a smaller, more efficient version of BERT, achieved through distillation, a technique that transfers knowledge from a larger, more complex model to a smaller one. This method allows for a significant reduction in size while maintaining performance.
Many other transformer-based architectures emerged, each offering improvements in various NLP tasks (e.g., XLNet, RoBERTa, ALBERT, T5).

## Vision Transformers (ViT):

Transformers were adapted for vision tasks with models like ViT, where an image is divided into patches, treated as sequences similar to text tokens, and passed through a transformer. This approach showed competitive performance compared to traditional Convolutional Neural Networks (CNNs) on image classification tasks.
The DeiT (Data-efficient Image Transformer) improved on this by using distillation to enhance performance without additional data.

## Multimodal Transformers:

The Perceiver architecture by DeepMind introduced a solution to the computational bottleneck of transformers, allowing them to handle long sequences from multiple modalities (e.g., text, images, audio) by focusing on a compact latent representation of the input.


### Large Models & Accuracy:
A billion-parameter model achieved over 90.4% top-1 accuracy on ImageNet, while a scaled-down version with only 10,000 images (just 10 per class) reached 84.8% accuracy.

### Transformer Improvements: 
In 2022, Mitchell Wortsman et al. showed that averaging the weights of multiple transformers can create a more powerful model, similar to an ensemble method, but without any inference penalty.

### Multimodal Transformers: 
The latest trend is building large multimodal transformers capable of zero-shot or few-shot learning. OpenAI‚Äôs CLIP, released in 2021, learned image representations by matching images with captions and can classify images based on simple text prompts. Shortly after, OpenAI launched DALL¬∑E, a model for generating images from text prompts, and DALL¬∑E 2, which uses a diffusion model for even higher-quality image generation.

### DeepMind‚Äôs Multimodal Models: 
DeepMind introduced Flamingo (April 2022), a model pretrained across multiple modalities, which can perform tasks like question answering and image captioning. They also introduced GATO (May 2022), a multimodal model capable of various tasks, including playing games, controlling robots, and generating captions, all with just 1.2 billion parameters.

### Pretrained Models: 
With the prevalence of transformer models, pretrained models are readily available from platforms like TensorFlow Hub and Hugging Face‚Äôs model hub, meaning you can often skip the implementation and use powerful pretrained models directly.

The text discusses Hugging Face's Transformers library, a popular open-source tool for working with transformer models, particularly in NLP, vision, and beyond. It allows easy downloading and fine-tuning of pretrained models, supporting frameworks like TensorFlow, PyTorch, and JAX.

Key points:

Pipeline API: Hugging Face offers the pipeline() function, simplifying model usage for tasks like sentiment analysis. Users can directly specify tasks (e.g., "sentiment-analysis") and the function downloads the appropriate pretrained model.

Example:


Manual Model and Tokenizer Loading: For more control, users can manually load models and tokenizers using AutoTokenizer and TFAutoModelForSequenceClassification, enabling custom pre-processing like tokenization and padding.

### Tokenization and Model Inference: 
After tokenizing text, the model can be used for inference to predict text classifications. Tokenization returns tensors which can be processed further to obtain predictions, with logits converted to probabilities.

### Fine-Tuning:
Users can fine-tune models on their datasets. A key requirement is using the correct loss function, like SparseCategoricalCrossentropy(from_logits=True), since models often output logits rather than probabilities. Hugging Face also provides tools to preprocess datasets using the Datasets library, which aids in tasks like downloading and masking data.

Learning Resources: Hugging Face's website offers extensive documentation, tutorials, and books for further learning. The "Natural Language Processing with Transformers" book by Hugging Face team members is recommended.

