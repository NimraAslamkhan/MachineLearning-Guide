{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Export the Model"
      ],
      "metadata": {
        "id": "71i6gV23ltrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "model_name = \"my_mnist_model\"\n",
        "model_version = \"0001\"\n",
        "model_path = Path(model_name) / model_version\n",
        "model.save(model_path, save_format=\"tf\")"
      ],
      "metadata": {
        "id": "ooUUVdiDl9dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspect the SavedModel"
      ],
      "metadata": {
        "id": "MbIJdgrmmC2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model_cli show --dir my_mnist_model/0001 --tag_set serve\n"
      ],
      "metadata": {
        "id": "6BJOLp5ImEWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Install TensorFlow Serving"
      ],
      "metadata": {
        "id": "zrg7NIXJmvw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt update -q && apt-get install -y tensorflow-model-server\n",
        "%pip install -q -U tensorflow-serving-api\n"
      ],
      "metadata": {
        "id": "hG7jpLThmz60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start the TF Serving Server\n",
        "## Start the server specifying:\n",
        "\n",
        "Model name and base path.\n",
        "Ports for gRPC (8500) and REST API (8501)."
      ],
      "metadata": {
        "id": "dTZMgrRWm2sE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensorflow_model_server \\\n",
        "  --port=8500 \\\n",
        "  --rest_api_port=8501 \\\n",
        "  --model_name=my_mnist_model \\\n",
        "  --model_base_path=\"/path/to/my_mnist_model\"\n"
      ],
      "metadata": {
        "id": "LuMK3QKcnBQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query the Model via REST API"
      ],
      "metadata": {
        "id": "HEj4URDSnGTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "request_json = json.dumps({\n",
        "    \"signature_name\": \"serving_default\",\n",
        "    \"instances\": X_test[:3].tolist()\n",
        "})\n"
      ],
      "metadata": {
        "id": "TN6SDuiHnH5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Send Request: Use the requests library to make a POST request to the server"
      ],
      "metadata": {
        "id": "7QkqTc4QnNk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "response = requests.post(\"http://localhost:8501/v1/models/my_mnist_model:predict\", data=request_json)\n",
        "response.raise_for_status()\n",
        "y_proba = np.array(response.json()[\"predictions\"]).round(2)\n"
      ],
      "metadata": {
        "id": "N2j8rQ-ZnPGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Querying TensorFlow Serving via gRPC API\n",
        "## Setup and Request Creation:\n",
        "\n",
        "Use PredictRequest from tensorflow_serving.apis.predict_pb2 to create a request.\n",
        "Specify the model name, signature, and input data using tf.make_tensor_proto.\n",
        "python\n",
        "Copy code\n"
      ],
      "metadata": {
        "id": "M1JQcRbqnc2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow_serving.apis.predict_pb2 import PredictRequest\n",
        "request = PredictRequest()\n",
        "request.model_spec.name = model_name\n",
        "request.model_spec.signature_name = \"serving_default\"\n",
        "request.inputs[input_name].CopyFrom(tf.make_tensor_proto(X_new))\n"
      ],
      "metadata": {
        "id": "pU8mnwLEnhlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Send Request and Receive Response:\n",
        "\n",
        "Establish a gRPC channel using grpc.\n",
        "Send the request with a timeout and handle the response.\n",
        "\n"
      ],
      "metadata": {
        "id": "IEJt6KnKnm4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import grpc\n",
        "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
        "channel = grpc.insecure_channel('localhost:8500')\n",
        "predict_service = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
        "response = predict_service.Predict(request, timeout=10.0)\n"
      ],
      "metadata": {
        "id": "-IdJtT8jntFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process the Response:\n",
        "\n",
        "Convert the response to a tensor using tf.make_ndarray."
      ],
      "metadata": {
        "id": "X4575vNOnwjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_proba = tf.make_ndarray(response.outputs[output_name])\n"
      ],
      "metadata": {
        "id": "WDEnOpxcnyvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ertex AI Setup\n",
        "## Prerequisites:\n",
        "\n",
        "Set up a Google Cloud account with billing enabled.\n",
        "Create a GCP project and activate necessary APIs (e.g., Cloud Storage, Vertex AI).\n",
        "## Authentication:\n",
        "\n",
        "Use google.colab.auth for OAuth2 authentication in Colab.\n",
        "\n"
      ],
      "metadata": {
        "id": "k1QhQl9Hn5uA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n"
      ],
      "metadata": {
        "id": "dKNKvz5Vn_P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a Google Cloud Storage Bucket:\n",
        "\n",
        "Use the google-cloud-storage library to store your SavedModels."
      ],
      "metadata": {
        "id": "_UUPJFZsoFng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "storage_client = storage.Client(project=project_id)\n",
        "bucket = storage_client.create_bucket(bucket_name, location=location)\n"
      ],
      "metadata": {
        "id": "LhExC7Z3oHQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Models in Web Pages with TensorFlow.js (TFJS)\n",
        "### Use Cases\n",
        "Offline Accessibility: Ideal for web apps with intermittent connectivity (e.g., hiking apps).\n",
        "### Low Latency:\n",
        "Reduces delay in real-time applications like online games.\n",
        "### Privacy Preservation:\n",
        "Keeps user data local for private predictions.\n",
        "Implementation Example\n",
        "Use TensorFlow.js to load and run models directly in the browser:"
      ],
      "metadata": {
        "id": "d5KQzbpEoV-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import \"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest\";\n",
        "import \"https://cdn.jsdelivr.net/npm/@tensorflow-models/mobilenet@1.0.0\";\n",
        "const image = document.getElementById(\"image\");\n",
        "mobilenet.load().then(model => {\n",
        "  model.classify(image).then(predictions => {\n",
        "    predictions.forEach(prediction => {\n",
        "      console.log(`${prediction.className}: ${(prediction.probability * 100).toFixed(1)}%`);\n",
        "    });\n",
        "  });\n",
        "});\n"
      ],
      "metadata": {
        "id": "1f_Sh6HbofmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuring TensorFlow to Use GPUs\n",
        "List Available GPUs"
      ],
      "metadata": {
        "id": "2to9Ae_BondT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "physical_gpus = tf.config.list_physical_devices('GPU')\n",
        "print(physical_gpus)\n"
      ],
      "metadata": {
        "id": "09zVTenbouZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Control GPU Memory Usage:\n",
        "\n",
        "Limit memory per GPU:"
      ],
      "metadata": {
        "id": "T2EwvYV7ot72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for gpu in physical_gpus:\n",
        "    tf.config.set_logical_device_configuration(\n",
        "        gpu,\n",
        "        [tf.config.LogicalDeviceConfiguration(memory_limit=2048)]\n",
        "    )\n"
      ],
      "metadata": {
        "id": "_IujpIwdo1Lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enable dynamic memory allocation:\n",
        "\n"
      ],
      "metadata": {
        "id": "_YHz1VP9o7NI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for gpu in physical_gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n"
      ],
      "metadata": {
        "id": "di3wll_Mo9jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the environment variable"
      ],
      "metadata": {
        "id": "t_wRQB4mpE5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "export TF_FORCE_GPU_ALLOW_GROWTH=true\n"
      ],
      "metadata": {
        "id": "igxkKggcpJc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cluster Specification\n",
        "You start by defining the cluster specification (cluster_spec), which includes the job types and task addresses for each machine involved. Here's an example of a cluster with two workers and one parameter server"
      ],
      "metadata": {
        "id": "LzJXgvxXpcbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_spec = {\n",
        "    \"worker\": [\n",
        "        \"machine-a.example.com:2222\",  # /job:worker/task:0\n",
        "        \"machine-b.example.com:2222\"   # /job:worker/task:1\n",
        "    ],\n",
        "    \"ps\": [\"machine-a.example.com:2221\"]  # /job:ps/task:0\n",
        "}\n"
      ],
      "metadata": {
        "id": "CsET0oj4peSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Starting a TensorFlow Cluster\n",
        "The TF_CONFIG environment variable is used to specify the configuration for each task (worker or parameter server). For example, to configure the first worker, you can set"
      ],
      "metadata": {
        "id": "PK3WdSPqpjpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "os.environ[\"TF_CONFIG\"] = json.dumps({\n",
        "    \"cluster\": cluster_spec,\n",
        "    \"task\": {\"type\": \"worker\", \"index\": 0}\n",
        "})\n"
      ],
      "metadata": {
        "id": "eetn9wswpm2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training with MultiWorkerMirroredStrategy\n",
        "When using multiple workers for training, you can use TensorFlow’s MultiWorkerMirroredStrategy for synchronous training across multiple devices. This ensures that each worker performs the same computation in parallel, and updates the model synchronously"
      ],
      "metadata": {
        "id": "UTjpxzEipuRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tempfile\n",
        "\n",
        "strategy = tf.distribute.MultiWorkerMirroredStrategy()  # The strategy\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()  # Resolver for cluster info\n",
        "print(f\"Starting task {resolver.task_type} #{resolver.task_id}\")\n",
        "\n",
        "# Load and split dataset\n",
        "with strategy.scope():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "    ])  # Define the model\n",
        "\n",
        "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10)\n",
        "\n",
        "# Chief saves the model, other workers save to temporary directories\n",
        "if resolver.task_id == 0:  # Chief saves the model\n",
        "    model.save(\"my_mnist_multiworker_model\", save_format=\"tf\")\n",
        "else:\n",
        "    tmpdir = tempfile.mkdtemp()  # Temporary directory for other workers\n",
        "    model.save(tmpdir, save_format=\"tf\")\n",
        "    tf.io.gfile.rmtree(tmpdir)  # Clean up\n"
      ],
      "metadata": {
        "id": "29ziVQNvpy1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Communication Options\n",
        "TensorFlow uses different communication algorithms (e.g., ring or NCCL) for synchronizing worker computations during training. You can explicitly set the communication strategy to use NCCL if needed for better performance on multi-GPU setup"
      ],
      "metadata": {
        "id": "3VG_kh_qp4zP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strategy = tf.distribute.MultiWorkerMirroredStrategy(\n",
        "    communication_options=tf.distribute.experimental.CommunicationOptions(\n",
        "        implementation=tf.distribute.experimental.CollectiveCommunication.NCCL\n",
        "    )\n",
        ")\n",
        ""
      ],
      "metadata": {
        "id": "Rc0Wm_iQp8a_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running on TPUs\n",
        "If you have access to TPUs (e.g., via Google Cloud), you can use the TPUStrategy"
      ],
      "metadata": {
        "id": "pP9mdwjiqC5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)  # Initialize the TPU system\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n"
      ],
      "metadata": {
        "id": "h3i9lmqbqF6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running on Google Cloud (Vertex AI)\n",
        "For large-scale training, you can use Vertex AI to run distributed training jobs on Google Cloud. This can be done using the aiplatform.CustomTrainingJob API to create and run custom training jobs on multiple workers with GPUs."
      ],
      "metadata": {
        "id": "bzEOeugIqLM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "custom_training_job = aiplatform.CustomTrainingJob(\n",
        "    display_name=\"my_custom_training_job\",\n",
        "    script_path=\"my_vertex_ai_training_task.py\",\n",
        "    container_uri=\"gcr.io/cloud-aiplatform/training/tf-gpu.2-4:latest\",\n",
        "    model_serving_container_image_uri=server_image,\n",
        "    requirements=[\"gcsfs==2022.3.0\"],  # Example\n",
        "    staging_bucket=f\"gs://{bucket_name}/staging\"\n",
        ")\n",
        "\n",
        "mnist_model = custom_training_job.run(\n",
        "    machine_type=\"n1-standard-4\",\n",
        "    replica_count=2,\n",
        "    accelerator_type=\"NVIDIA_TESLA_K80\",\n",
        "    accelerator_count=2,\n",
        ")\n"
      ],
      "metadata": {
        "id": "NJnOpJGQqPwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning with Vertex AI\n",
        "For optimizing hyperparameters, Vertex AI provides a hyperparameter tuning service that uses Bayesian optimization to efficiently search the hyperparameter space. You can pass hyperparameters as command-line arguments and use them in your training scrip"
      ],
      "metadata": {
        "id": "XS0uMn8KqVSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import tensorflow as tf\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--n_hidden\", type=int, default=2)\n",
        "parser.add_argument(\"--n_neurons\", type=int, default=256)\n",
        "parser.add_argument(\"--learning_rate\", type=float, default=1e-2)\n",
        "parser.add_argument(\"--optimizer\", default=\"adam\")\n",
        "args = parser.parse_args()\n",
        "\n",
        "def build_model(args):\n",
        "    with tf.distribute.MirroredStrategy().scope():\n",
        "        model = tf.keras.Sequential()\n",
        "        model.add(tf.keras.layers.Flatten(input_shape=[28, 28], dtype=tf.uint8))\n",
        "        for _ in range(args.n_hidden):\n",
        "            model.add(tf.keras.layers.Dense(args.n_neurons, activation=\"relu\"))\n",
        "        model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
        "        opt = tf.keras.optimizers.get(args.optimizer)\n",
        "        opt.learning_rate = args.learning_rate\n",
        "        model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "model = build_model(args)\n",
        "model.fit(X_train, y_train, epochs=10)\n"
      ],
      "metadata": {
        "id": "wQ4_Eh5MqXXK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}