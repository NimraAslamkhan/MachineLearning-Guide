{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Losses, Metrics, and Gradients"
      ],
      "metadata": {
        "id": "S9OzGbns3HHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Define a simple regression model with auxiliary outputs\n",
        "inputs = tf.keras.Input(shape=(10,))\n",
        "hidden = tf.keras.layers.Dense(5, activation=\"relu\")(inputs)\n",
        "outputs = tf.keras.layers.Dense(1)(hidden)"
      ],
      "metadata": {
        "id": "o_vZCSbS2gTD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = tf.keras.Model(inputs, outputs)\n"
      ],
      "metadata": {
        "id": "5FdvWmMF3NNE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define a simple regression model\n",
        "inputs = tf.keras.Input(shape=(10,))\n",
        "hidden = tf.keras.layers.Dense(5, activation=\"relu\", name=\"hidden_layer\")(inputs)\n",
        "outputs = tf.keras.layers.Dense(1, name=\"output_layer\")(hidden)"
      ],
      "metadata": {
        "id": "nieVgJNc3TS1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Define a custom loss function as a Keras layer\n",
        "class ReconstructionLossLayer(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        return tf.reduce_mean(tf.square(inputs))"
      ],
      "metadata": {
        "id": "m9LQCn1j7lNC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the custom loss layer\n",
        "reconstruction_loss_layer = ReconstructionLossLayer()\n",
        "\n",
        "# Add the reconstruction loss using the hidden layer\n",
        "def reconstruction_loss(hidden):\n",
        "    return reconstruction_loss_layer(hidden)\n"
      ],
      "metadata": {
        "id": "pguB5KyU7nQm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap hidden layer computation for loss calculation\n",
        "hidden_output = model.get_layer(\"hidden_layer\").output\n",
        "recon_loss = reconstruction_loss(hidden_output)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G5YN0kOl7wI6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "# Define a custom loss layer\n",
        "class ReconstructionLossLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(ReconstructionLossLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.reduce_mean(tf.square(inputs))\n",
        "\n",
        "# Define the model\n",
        "inputs = tf.keras.Input(shape=(10,))\n",
        "hidden = tf.keras.layers.Dense(5, activation=\"relu\", name=\"hidden_layer\")(inputs)\n",
        "outputs = tf.keras.layers.Dense(1, name=\"output_layer\")(hidden)\n",
        "\n",
        "# Instantiate the reconstruction loss layer\n",
        "reconstruction_loss_layer = ReconstructionLossLayer()\n",
        "\n",
        "# Use the hidden layer's output for reconstruction loss\n",
        "reconstruction_loss = reconstruction_loss_layer(hidden)\n",
        "\n",
        "# Create the model\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Define a combined loss function\n",
        "def combined_loss(y_true, y_pred):\n",
        "    mse_loss = tf.reduce_mean(tf.square(y_true - y_pred))  # MSE loss\n",
        "    total_loss = mse_loss + 0.01 * reconstruction_loss  # Add reconstruction loss\n",
        "    return total_loss\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=combined_loss, metrics=[\"mae\"])\n",
        "\n",
        "# Generate synthetic data\n",
        "X_train = tf.random.normal((100, 10))\n",
        "y_train = tf.random.normal((100, 1))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wpSVZ3EA8trz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "# Define a custom layer for reconstruction loss\n",
        "class ReconstructionLossLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Compute the reconstruction loss\n",
        "        return tf.reduce_mean(tf.square(inputs))\n",
        "\n",
        "# Define the model\n",
        "inputs = tf.keras.Input(shape=(10,), name=\"input_layer\")\n",
        "hidden = tf.keras.layers.Dense(5, activation=\"relu\", name=\"hidden_layer\")(inputs)\n",
        "reconstruction_loss = ReconstructionLossLayer()(hidden)  # Use custom layer\n",
        "outputs = tf.keras.layers.Dense(1, name=\"output_layer\")(hidden)\n",
        "\n",
        "# Create the model\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "\n",
        "# Generate synthetic data\n",
        "X_train = tf.random.normal((100, 10))  # 100 samples, 10 features\n",
        "y_train = tf.random.normal((100, 1))   # 100 samples, 1 output\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=16)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u16IpaAJ8ysc",
        "outputId": "7357c865-dc49-4c73-e76b-f15a161c75af"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.3795 - mae: 0.9855  \n",
            "Epoch 2/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1823 - mae: 0.8909  \n",
            "Epoch 3/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1175 - mae: 0.8605 \n",
            "Epoch 4/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2197 - mae: 0.9133 \n",
            "Epoch 5/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2481 - mae: 0.9134 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7bd3fc1aea10>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customizable Loss with Parameters\n",
        "To allow custom thresholds in Huber loss:"
      ],
      "metadata": {
        "id": "1sEd6300GdnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_huber(threshold=1.0):\n",
        "    def huber_fn(y_true, y_pred):\n",
        "        error = y_true - y_pred\n",
        "        is_small_error = tf.abs(error) < threshold\n",
        "        squared_loss = tf.square(error) / 2\n",
        "        linear_loss = threshold * tf.abs(error) - threshold ** 2 / 2\n",
        "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "    return huber_fn\n"
      ],
      "metadata": {
        "id": "LboMjgc7Ckvy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=create_huber(2.0), optimizer=\"nadam\")\n"
      ],
      "metadata": {
        "id": "o4aUh7eoCrAQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subclassing for Persistent Custom Loss\n",
        "To save threshold values within the model:"
      ],
      "metadata": {
        "id": "YCnmWo48Gl-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HuberLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, threshold=1.0, **kwargs):\n",
        "        self.threshold = threshold\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        error = y_true - y_pred\n",
        "        is_small_error = tf.abs(error) < self.threshold\n",
        "        squared_loss = tf.square(error) / 2\n",
        "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
        "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, \"threshold\": self.threshold}\n"
      ],
      "metadata": {
        "id": "9G7K6ZM-C20h"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Custom Functions\n",
        "You can create custom implementations for:\n",
        "\n",
        "Activation Functions: Define how neurons activate."
      ],
      "metadata": {
        "id": "QUInkojsDIka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_softplus(z):\n",
        "    return tf.math.log(1.0 + tf.exp(z))\n"
      ],
      "metadata": {
        "id": "KIIJ6X2ADL1F"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializers: Customize weight initialization."
      ],
      "metadata": {
        "id": "f9aKo_nNDPaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_glorot_initializer(shape, dtype=tf.float32):\n",
        "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
        "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n"
      ],
      "metadata": {
        "id": "wfRlR5pcDUQA"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularizers: Add penalties to model weights to prevent overfitting"
      ],
      "metadata": {
        "id": "kRswigNkDW87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_l1_regularizer(weights):\n",
        "    return tf.reduce_sum(tf.abs(0.01 * weights))\n"
      ],
      "metadata": {
        "id": "Z8YPvrFJDbo1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Constraints: Enforce specific conditions on weights (e.g., non-negativity)"
      ],
      "metadata": {
        "id": "8nffkxLXDeR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_positive_weights(weights):\n",
        "    return tf.where(weights < 0., tf.zeros_like(weights), weights)\n"
      ],
      "metadata": {
        "id": "2GbN81hZDl_n"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usage in Layers: These functions are seamlessly integrated into layers:"
      ],
      "metadata": {
        "id": "dUTs4sceDo1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer = tf.keras.layers.Dense(1,\n",
        "                              activation=my_softplus,\n",
        "                              kernel_initializer=my_glorot_initializer,\n",
        "                              kernel_regularizer=my_l1_regularizer,\n",
        "                              kernel_constraint=my_positive_weights)\n"
      ],
      "metadata": {
        "id": "9X5nrsPTDtAW"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Custom Classes\n",
        "For functions with hyperparameters, use specialized Keras classes like:\n",
        "\n",
        "Custom Regularizers:"
      ],
      "metadata": {
        "id": "4N9hmeHODvvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyL1Regularizer(tf.keras.regularizers.Regularizer):\n",
        "    def __init__(self, factor):\n",
        "        self.factor = factor\n",
        "\n",
        "    def __call__(self, weights):\n",
        "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"factor\": self.factor}\n"
      ],
      "metadata": {
        "id": "Xyofm7pmD0HR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Custom Metrics\n",
        "Metrics differ from losses:\n",
        "\n",
        "Losses: Differentiable, for optimization.\n",
        "Metrics: Not necessarily differentiable, for evaluation.\n",
        "For example, a stateful metric like precision:"
      ],
      "metadata": {
        "id": "unHbRzVbD2ZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "precision = tf.keras.metrics.Precision()\n",
        "precision([0, 1, 1, 0], [1, 1, 0, 0])\n",
        "precision.result().numpy()  # Returns precision after multiple batches.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl8Obt0GD8rO",
        "outputId": "c2511cf6-af66-4b37-a8f5-677472c43314"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Metric Example:\n",
        "\n",
        "Stateful metric for mean Huber loss:"
      ],
      "metadata": {
        "id": "0Vc7yWqID__A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HuberMetric(tf.keras.metrics.Metric):\n",
        "    def __init__(self, threshold=1.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.threshold = threshold\n",
        "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
        "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        sample_metrics = tf.where(\n",
        "            tf.abs(y_true - y_pred) < self.threshold,\n",
        "            0.5 * tf.square(y_true - y_pred),\n",
        "            self.threshold * tf.abs(y_true - y_pred) - 0.5 * self.threshold**2)\n",
        "        self.total.assign_add(tf.reduce_sum(sample_metrics))\n",
        "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        return self.total / self.count\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"threshold\": self.threshold}\n"
      ],
      "metadata": {
        "id": "wKfNt_ETEDq4"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Layers\n",
        "To build new layers:\n",
        "\n",
        "Subclass tf.keras.layers.Layer.\n",
        "Define:\n",
        "__init__(): Layer hyperparameters.\n",
        "build(): Create weights.\n",
        "call(): Layer's forward pass logic."
      ],
      "metadata": {
        "id": "y9KB0wllEIuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDense(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, activation=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(\n",
        "            name=\"kernel\", shape=[input_shape[-1], self.units], initializer=\"glorot_normal\")\n",
        "        self.bias = self.add_weight(\n",
        "            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
        "\n",
        "    def call(self, X):\n",
        "        return self.activation(X @ self.kernel + self.bias)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"units\": self.units, \"activation\": self.activation}\n"
      ],
      "metadata": {
        "id": "E6MLaWA7ENeh"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing Gradients with Autodiff\n",
        "Automatic Gradient Calculation\n",
        "Use tf.GradientTape for automatic differentiation."
      ],
      "metadata": {
        "id": "lHs4f9KTEZJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a toy function\n",
        "def f(w1, w2):\n",
        "    return w1**2 + w2**3\n",
        "\n",
        "# Compute gradients\n",
        "w1 = tf.Variable(2.0)\n",
        "w2 = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    loss = f(w1, w2)\n",
        "\n",
        "gradients = tape.gradient(loss, [w1, w2])\n",
        "print(\"Gradients:\", gradients)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0P6jrPTEblR",
        "outputId": "9a335e76-deb5-4c66-fe3f-902ffca9b198"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients: [<tf.Tensor: shape=(), dtype=float32, numpy=4.0>, <tf.Tensor: shape=(), dtype=float32, numpy=27.0>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Persistent Gradient Calculation\n",
        "For multiple gradient calculations, use persistent=True."
      ],
      "metadata": {
        "id": "ojkRke9YEh_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.GradientTape(persistent=True) as tape:\n",
        "    x = tf.Variable(5.0)\n",
        "    y = x**2 + 3*x\n",
        "\n",
        "grad_x = tape.gradient(y, x)  # First calculation\n",
        "grad_x_again = tape.gradient(grad_x, x)  # Second calculation\n",
        "del tape  # Release resources\n"
      ],
      "metadata": {
        "id": "ttKezyzvEjnw"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customizing What to Track\n",
        "Use tape.watch() to track non-variable tensors."
      ],
      "metadata": {
        "id": "MdOqeOHDEmZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.constant(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    tape.watch(x)\n",
        "    y = x**3\n",
        "\n",
        "grad = tape.gradient(y, x)\n",
        "print(\"Gradient:\", grad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4QET2ZGErUH",
        "outputId": "f3392ce7-de88-4567-f80e-72d7e340e831"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient: tf.Tensor(27.0, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Gradient Techniques\n",
        "Gradient of Summed Losses\n",
        "By default, TensorFlow computes gradients of summed losses. Use tape.jacobian() for individual gradients."
      ],
      "metadata": {
        "id": "PgPOs-YbEurQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.GradientTape() as tape:\n",
        "    x = tf.constant([1.0, 2.0])\n",
        "    y = x**2\n",
        "\n",
        "jacobian = tape.jacobian(y, x)\n",
        "print(\"Jacobian:\", jacobian)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2muE9WoEzv1",
        "outputId": "9cb3ed4a-cf99-4884-8f6a-73c72dc55202"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jacobian: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopping Gradients\n",
        "Use tf.stop_gradient() to prevent backpropagation."
      ],
      "metadata": {
        "id": "BDU-XuioE3Aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y = tf.stop_gradient(x) * x\n",
        "\n",
        "grad = tape.gradient(y, x)\n",
        "print(\"Gradient:\", grad)  # Should be 0 due to stop_gradient\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmqpVITsE9Lu",
        "outputId": "3dfa844a-62e0-4e90-eed8-3fe917b1f635"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient: tf.Tensor(3.0, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Numerical Stability\n",
        "Common Issues\n",
        "Operations like square roots or exponentials can cause instability for extreme values, resulting in infinities or NaNs.\n",
        "\n",
        "Solutions\n",
        "Modify operations to ensure stability."
      ],
      "metadata": {
        "id": "ZFPdwQy0E_-G"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VzErZ7zaE_pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stable computation of log(1 + exp(z))\n",
        "def stable_log_exp(z):\n",
        "    return tf.math.log1p(tf.math.exp(-tf.abs(z))) + tf.maximum(z, 0)\n",
        "\n",
        "z = tf.constant([100.0, -100.0])\n",
        "result = stable_log_exp(z)\n",
        "print(\"Stable Output:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRD8CdZ7E_mG",
        "outputId": "77057fee-2275-4daa-a134-5d497ac3904a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stable Output: tf.Tensor([100.   0.], shape=(2,), dtype=float32)\n"
          ]
        }
      ]
    }
  ]
}