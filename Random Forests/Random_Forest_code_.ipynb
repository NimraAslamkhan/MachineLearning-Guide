{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# how to create a voting classifier using Scikit-Learn's VotingClassifier to combine multiple classifiers and evaluate their performance on a synthetic dataset."
      ],
      "metadata": {
        "id": "NIM7l-F_hI3H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvTncD1NgxY-",
        "outputId": "3ddef27e-1acb-479c-ceac-cdd6e388ed01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr = 0.864\n",
            "rf = 0.896\n",
            "svc = 0.896\n",
            "Voting Classifier prediction: [1]\n",
            "Individual predictions: [array([1]), array([1]), array([0])]\n",
            "Voting Classifier (hard voting) accuracy: 0.912\n",
            "Voting Classifier (soft voting) accuracy: 0.92\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate the dataset\n",
        "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Create a Voting Classifier with three diverse classifiers\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('lr', LogisticRegression(random_state=42)),\n",
        "        ('rf', RandomForestClassifier(random_state=42)),\n",
        "        ('svc', SVC(probability=True, random_state=42))\n",
        "    ],\n",
        "    voting='hard'  # Start with hard voting\n",
        ")\n",
        "\n",
        "# Train the Voting Classifier\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate individual classifiers\n",
        "for name, clf in voting_clf.named_estimators_.items():\n",
        "    print(name, \"=\", clf.score(X_test, y_test))\n",
        "\n",
        "# Predict using the voting classifier\n",
        "print(\"Voting Classifier prediction:\", voting_clf.predict(X_test[:1]))\n",
        "print(\"Individual predictions:\", [clf.predict(X_test[:1]) for clf in voting_clf.estimators_])\n",
        "\n",
        "# Evaluate Voting Classifier's performance (hard voting)\n",
        "print(\"Voting Classifier (hard voting) accuracy:\", voting_clf.score(X_test, y_test))\n",
        "\n",
        "# Change to soft voting and re-train\n",
        "voting_clf.voting = \"soft\"\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate Voting Classifier's performance (soft voting)\n",
        "print(\"Voting Classifier (soft voting) accuracy:\", voting_clf.score(X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bagging and Pasting"
      ],
      "metadata": {
        "id": "KIRtlVHghQK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Create a BaggingClassifier with DecisionTreeClassifier as the base estimator\n",
        "bag_clf = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),  # Use 'estimator' instead of 'base_estimator'\n",
        "    n_estimators=500,\n",
        "    max_samples=100,\n",
        "    n_jobs=-1,  # Use all available cores\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the BaggingClassifier on the training set\n",
        "bag_clf.fit(X_train, y_train)\n",
        "\n",
        "# Perform Out-of-Bag evaluation\n",
        "bag_clf_oob = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),  # Use 'estimator' instead of 'base_estimator'\n",
        "    n_estimators=500,\n",
        "    oob_score=True,  # Enable OOB evaluation\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the BaggingClassifier with OOB evaluation\n",
        "bag_clf_oob.fit(X_train, y_train)\n",
        "\n",
        "# Retrieve the OOB score\n",
        "oob_score = bag_clf_oob.oob_score_\n",
        "print(\"OOB Score:\", oob_score)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = bag_clf_oob.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Set Accuracy:\", accuracy)\n",
        "\n",
        "# Get OOB decision function probabilities for the first 3 instances\n",
        "oob_decision_function = bag_clf_oob.oob_decision_function_[:3]\n",
        "print(\"OOB Decision Function Probabilities for the first 3 instances:\\n\", oob_decision_function)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8aT28Z4h6ef",
        "outputId": "84db9c3d-2bad-4bed-9ca7-f14b984aabe7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOB Score: 0.896\n",
            "Test Set Accuracy: 0.92\n",
            "OOB Decision Function Probabilities for the first 3 instances:\n",
            " [[0.32352941 0.67647059]\n",
            " [0.3375     0.6625    ]\n",
            " [1.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This code includes training a RandomForestClassifier and a BaggingClassifier"
      ],
      "metadata": {
        "id": "k_PxCLJTiTpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris(as_frame=True)\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Train a RandomForestClassifier\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\n",
        "rnd_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the Random Forest model\n",
        "y_pred_rf = rnd_clf.predict(X_test)\n",
        "\n",
        "# Display feature importances\n",
        "print(\"Feature Importances from Random Forest Classifier:\")\n",
        "for score, name in zip(rnd_clf.feature_importances_, iris.data.columns):\n",
        "    print(round(score, 2), name)\n",
        "\n",
        "# Equivalent BaggingClassifier using DecisionTreeClassifier\n",
        "bag_clf = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(max_features=\"sqrt\", max_leaf_nodes=16),\n",
        "    n_estimators=500,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the BaggingClassifier\n",
        "bag_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the Bagging model\n",
        "y_pred_bag = bag_clf.predict(X_test)\n",
        "\n",
        "# Display predictions\n",
        "print(\"\\nPredictions from Bagging Classifier:\", y_pred_bag)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYNdU84Wh8X2",
        "outputId": "8865c4b5-5d9d-46b1-d817-72de2b1cb74f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances from Random Forest Classifier:\n",
            "0.11 sepal length (cm)\n",
            "0.03 sepal width (cm)\n",
            "0.44 petal length (cm)\n",
            "0.41 petal width (cm)\n",
            "\n",
            "Predictions from Bagging Classifier: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
            " 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Boosting (AdaBoost)"
      ],
      "metadata": {
        "id": "yt7CaHwYjaXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "# Generate the moons dataset\n",
        "X, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Create and train an AdaBoostClassifier\n",
        "ada_clf = AdaBoostClassifier(\n",
        "    estimator=DecisionTreeClassifier(max_depth=1),  # Changed from base_estimator to estimator\n",
        "    n_estimators=30,\n",
        "    learning_rate=0.5,\n",
        "    random_state=42\n",
        ")\n",
        "ada_clf.fit(X_train, y_train)\n",
        "\n",
        "# Score the AdaBoostClassifier\n",
        "print(\"AdaBoost Classifier Score:\", ada_clf.score(X_test, y_test))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugUGB2TrjbyJ",
        "outputId": "65e3f6f2-642a-482a-b352-d5ba948dcdea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Score: 0.904\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Boosting"
      ],
      "metadata": {
        "id": "bS4GADomj4hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Generate a noisy quadratic dataset\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) - 0.5\n",
        "y = 3 * X[:, 0] ** 2 + 0.05 * np.random.randn(100)  # y = 3x² + Gaussian noise\n",
        "\n",
        "# Train the first DecisionTreeRegressor\n",
        "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
        "tree_reg1.fit(X, y)\n",
        "\n",
        "# Train a second DecisionTreeRegressor on the residuals\n",
        "y2 = y - tree_reg1.predict(X)\n",
        "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=43)\n",
        "tree_reg2.fit(X, y2)\n",
        "\n",
        "# Train a third regressor on the residuals\n",
        "y3 = y2 - tree_reg2.predict(X)\n",
        "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=44)\n",
        "tree_reg3.fit(X, y3)\n",
        "\n",
        "# Predictions for new data\n",
        "X_new = np.array([[-0.4], [0.], [0.5]])\n",
        "predictions = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
        "print(\"Predictions:\", predictions)\n",
        "\n",
        "# Train a Gradient Boosting Regressor\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)\n",
        "gbrt.fit(X, y)\n",
        "\n",
        "# Early stopping with Gradient Boosting Regressor\n",
        "gbrt_best = GradientBoostingRegressor(\n",
        "    max_depth=2,\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=500,\n",
        "    n_iter_no_change=10,\n",
        "    random_state=42\n",
        ")\n",
        "gbrt_best.fit(X, y)\n",
        "\n",
        "# Display the number of estimators used\n",
        "print(\"Number of estimators after early stopping:\", gbrt_best.n_estimators_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6H415GTjr3h",
        "outputId": "5a41db17-ec83-4b6d-f11d-31f8b73d1a4c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [0.49484029 0.04021166 0.75026781]\n",
            "Number of estimators after early stopping: 92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Pipeline with HistGradientBoosting"
      ],
      "metadata": {
        "id": "Vf_i9GV4j-ge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# Load the California housing dataset\n",
        "housing = fetch_california_housing(as_frame=True).data\n",
        "housing_labels = fetch_california_housing(as_frame=True).target\n",
        "\n",
        "# Create a pipeline with HistGradientBoostingRegressor\n",
        "hgb_reg = make_pipeline(\n",
        "    make_column_transformer((OrdinalEncoder(), [\"ocean_proximity\"]), remainder=\"passthrough\"),\n",
        "    HistGradientBoostingRegressor(categorical_features=[0], random_state=42)\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mLoUtyXKkTVk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stacking Classifier"
      ],
      "metadata": {
        "id": "-FPsjzHGki-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Create a Stacking Classifier\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('lr', LogisticRegression(random_state=42)),\n",
        "        ('rf', RandomForestClassifier(random_state=42)),\n",
        "        ('svc', SVC(probability=True, random_state=42))\n",
        "    ],\n",
        "    final_estimator=RandomForestClassifier(random_state=43),\n",
        "    cv=5  # number of cross-validation folds\n",
        ")\n",
        "\n",
        "# Fit the Stacking Classifier\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Score the Stacking Classifier\n",
        "print(\"Stacking Classifier Score:\", stacking_clf.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QVpO__YkeBb",
        "outputId": "236d2b8e-bf5d-4478-9417-7bcb9b0f6f96"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Classifier Score: 0.928\n"
          ]
        }
      ]
    }
  ]
}