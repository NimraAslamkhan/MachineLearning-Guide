{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Dimensionality Reduction Basics\n",
        "\n",
        "## Purpose\n",
        "Reducing the dimensionality of a dataset can simplify models, reduce computation costs, and help with visualizing data. However, it may also lead to some loss of information.\n",
        "## Projection and Manifold Learning\n",
        "\n",
        "Projection: Projects data from a high-dimensional space into a lower-dimensional space. Most effective when data is roughly linear.\n",
        "Manifold Learning: Effective for complex, nonlinear structures. Instead of projecting, it finds a lower-dimensional manifold within the high-dimensional space.\n"
      ],
      "metadata": {
        "id": "cUNQ0fn8HwvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Principal Component Analysis (PCA)\n",
        "\n",
        "## Purpose\n",
        " PCA is a linear algorithm that aims to capture the maximum variance in the data by transforming the data into a new coordinate system using “principal components.\n",
        "\n",
        "## Principal Components\n",
        " Each principal component represents a direction in which data variance is maximized. The first component captures the most variance, and each subsequent component captures progressively less.\n",
        "\n",
        "## Explained Variance Ratio\n",
        " The ratio shows how much variance is explained by each principal component, helping decide the right number of components.\n",
        "\n",
        "## PCA for Compression\n",
        " By retaining only a few principal components, you can compress data effectively."
      ],
      "metadata": {
        "id": "5mQQYTcvH4lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load example dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "\n",
        "# Set the number of principal components\n",
        "pca = PCA(n_components=2)\n",
        "X_reduced = pca.fit_transform(X)\n",
        "\n",
        "# Explained variance ratio\n",
        "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlyp0jV5II8g",
        "outputId": "37302dc8-1e17-42c7-f794-ab85eed99072"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio: [0.92461872 0.05306648]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Randomized PCA\n",
        "\n",
        " An approximate version of PCA, faster for large dataset"
      ],
      "metadata": {
        "id": "JMRuePHuIIZC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Em5oEbFVHtZz"
      },
      "outputs": [],
      "source": [
        "pca_random = PCA(n_components=2, svd_solver='randomized')\n",
        "X_reduced_random = pca_random.fit_transform(X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Random Projection\n",
        "\n",
        "## Purpose\n",
        " Reduces dimensionality by projecting data into a lower-dimensional subspace using random linear mappings. It’s useful for high-dimensional data but may be less accurate for smaller dimensions."
      ],
      "metadata": {
        "id": "Z_XoSPN4IYMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "\n",
        "rp = GaussianRandomProjection(n_components=2)\n",
        "X_projected = rp.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "geyzmosKIh-A"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Locally Linear Embedding (LLE)\n",
        "\n",
        "## Purpose\n",
        " LLE is a nonlinear dimensionality reduction technique that uses local linear relationships to reduce dimensionality. It’s ideal for data lying on a nonlinear manifold.\n",
        "\n",
        "## Implementation\n",
        "Unlike PCA, LLE does not rely on projections but preserves relationships by reconstructing data points from their neighbors."
      ],
      "metadata": {
        "id": "_Ewqh_8JInfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import LocallyLinearEmbedding\n",
        "\n",
        "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
        "X_reduced_lle = lle.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "R9Xzlxi2IxLV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Other Dimensionality Reduction Techniques\n",
        "\n",
        "## Multidimensional Scaling (MDS)\n",
        "\n",
        "## Purpose\n",
        "MDS reduces dimensionality while preserving pairwise distances between points. Often used for visualization"
      ],
      "metadata": {
        "id": "4PcTqko2IzuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import MDS\n",
        "\n",
        "mds = MDS(n_components=2)\n",
        "X_reduced_mds = mds.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "x6s0JVW3JBWW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Isomap\n",
        "Purpose: Similar to LLE, Isomap preserves geodesic distances (distances measured along the manifold surface). Useful for nonlinear manifolds."
      ],
      "metadata": {
        "id": "jp86dkZEJHyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import Isomap\n",
        "\n",
        "isomap = Isomap(n_components=2, n_neighbors=5)\n",
        "X_reduced_isomap = isomap.fit_transform(X)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fwk9KTh2JHHV",
        "outputId": "e8602c43-6e2c-4175-c4e2-fb3c284350c2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_isomap.py:383: UserWarning: The number of connected components of the neighbors graph is 2 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
            "  self._fit_transform(X)\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/sparse/_index.py:108: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_intXint(row, col, x.flat[0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "\n",
        "Purpose: Primarily for visualization, t-SNE clusters similar points together while pushing dissimilar points apart. Ideal for high-dimensional data with complex clusters."
      ],
      "metadata": {
        "id": "xGW9wpaKJSWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(n_components=2)\n",
        "X_reduced_tsne = tsne.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "_KYamUIpJWNi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Discriminant Analysis (LDA)\n",
        "\n",
        "## Purpose\n",
        " Though primarily a classification tool, LDA can be used to reduce dimensionality by projecting data in a way that maximizes class separability. Typically, the number of components is one less than the number of classes.\n",
        "\n",
        "Note: LDA is supervised, so it requires class labels"
      ],
      "metadata": {
        "id": "9i0i11W1JdRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "\n",
        "# Assuming the dataset has target labels\n",
        "y = data.target\n",
        "lda = LDA(n_components=2)\n",
        "X_reduced_lda = lda.fit_transform(X, y)\n"
      ],
      "metadata": {
        "id": "HHpOo3l7Jj4x"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}